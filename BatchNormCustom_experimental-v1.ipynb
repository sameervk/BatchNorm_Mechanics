{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * data seems decently balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n",
       " array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(3,3)\n",
    "ax = ax.flatten()\n",
    "for i,j in enumerate(ax):\n",
    "    \n",
    "    j.imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.reshape(-1, 28*28)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape(-1, 28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler_fit = stdscaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = stdscaler_fit.transform(x_train)\n",
    "x_test_scaled = stdscaler_fit.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_train_scaled.astype(dtype = 'float32', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_scaled = x_test_scaled.astype(dtype = 'float32', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244.94693"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.274208"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot_train = tf.keras.utils.to_categorical(y_train, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_onehot_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "y_onehot_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot_test = tf.keras.utils.to_categorical(y_test, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random_seed = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled, y_onehot_train = shuffle(x_train_scaled, y_onehot_train, random_state = random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_scaled = x_train_scaled[:5000]\n",
    "y_onehot_valid =  y_onehot_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_train_scaled[5000:]\n",
    "y_onehot_train =  y_onehot_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tf.data.Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_scaled, y_onehot_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_scaled, y_onehot_test))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_scaled, y_onehot_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch = 60 # used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffersize = len(y_train)\n",
    "buffersize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size=buffersize, seed=random_seed,\n",
    "                                     reshuffle_each_iteration=True).batch(minibatch, \n",
    "                                                                          drop_remainder = True).prefetch(buffer_size=\n",
    "                                                                                                          tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_dataset.shuffle(buffer_size=buffersize, seed=random_seed, \n",
    "                                      reshuffle_each_iteration=False).batch(batch_size=minibatch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=buffersize, seed=random_seed, \n",
    "                                      reshuffle_each_iteration=False).batch(batch_size=minibatch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in test_dataset.take(1):\n",
    "    print(i[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating custom layers and keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * 3 dense layers followed by 3 batch norm layers and a final dense layer with units equal to number of categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Custom Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dense_layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units, **kwargs):\n",
    "        \n",
    "        super(dense_layer, self).__init__(**kwargs)\n",
    "        self.units =  units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.kernel_weights = self.add_weight(name = 'weights', shape = (input_shape[-1], self.units), \n",
    "                                              dtype = self.dtype, initializer=tf.keras.initializers.Orthogonal(gain=1,seed=random_seed),\n",
    "                                             trainable = self.trainable)\n",
    "        self.bias = self.add_weight(name = 'bias', shape = (self.units,), dtype = self.dtype,\n",
    "                                   initializer=tf.keras.initializers.zeros(), trainable = self.trainable)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return tf.add(tf.matmul(inputs, self.kernel_weights), self.bias)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(dense_layer, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0., 1., 2., 3., 4.],\n",
       "       [5., 6., 7., 8., 9.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =  tf.reshape(tf.range(0,10, dtype = 'float32'), (2,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlayer = dense_layer(10, name = 'test', dtype = x.dtype, trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.39185402, -0.8637892 , -3.5154657 ,  0.6156957 ,  1.3804076 ,\n",
       "         0.5214603 ,  0.07916665,  3.065235  ,  1.5912303 ,  1.5004475 ],\n",
       "       [-0.7935976 ,  0.40243608, -8.864796  ,  3.0001144 ,  4.0484476 ,\n",
       "        -3.460637  ,  1.753124  , 10.487197  ,  3.8650322 ,  3.2037134 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'test/weights:0' shape=(5, 10) dtype=float32, numpy=\n",
       " array([[ 0.46062446,  0.4805145 ,  0.27113432,  0.24228814,  0.03775417,\n",
       "         -0.36714008,  0.31868806,  0.41027665,  0.03715476,  0.13125132],\n",
       "        [-0.50286615,  0.16102067, -0.23385325,  0.2981111 ,  0.35705572,\n",
       "         -0.46852738, -0.22770971,  0.17325218, -0.32941478, -0.20062807],\n",
       "        [-0.3181508 , -0.21783853, -0.24576081, -0.13318287, -0.15441617,\n",
       "         -0.34629565,  0.48582542,  0.16841805,  0.5900791 , -0.12977235],\n",
       "        [ 0.37286136, -0.09267335, -0.65545404, -0.30528104, -0.15932733,\n",
       "         -0.14040446, -0.30327463,  0.3746355 , -0.11272153,  0.19858865],\n",
       "        [-0.09281758, -0.0777782 , -0.20593223,  0.37494838,  0.45254156,\n",
       "          0.5259481 ,  0.06126237,  0.35781002,  0.26966286,  0.34121358]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'test/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlayer.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Variables can have the same name if they are of different shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'test', 'trainable': True, 'dtype': 'float32', 'units': 10}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlayer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *End Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating custom batch normalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing Population mean and variance implemented in the paper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing variance\n",
    "x1 = np.random.randint(low = 0, high = 50, size = 12)\n",
    "x2 = np.random.randint(low = 0, high = 50, size = 12)\n",
    "x3 = np.random.randint(low = 0, high = 50, size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.5 209.08333333333334\n"
     ]
    }
   ],
   "source": [
    "print(x1.mean(), x1.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.25 155.52083333333334\n"
     ]
    }
   ],
   "source": [
    "print(x2.mean(), x2.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 191.41666666666666\n"
     ]
    }
   ],
   "source": [
    "print(x3.mean(), x3.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 44, 44, 41, 31,  9,  9, 15, 45, 23, 16,  5, 26, 38, 24, 31,  3,\n",
       "       49, 11, 10, 32, 22, 19, 14, 17,  0, 40, 33, 14, 22, 14, 37, 38, 31,\n",
       "       36,  0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((x1, x2, x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.083333333333332"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# population mean\n",
    "np.hstack((x1, x2, x3)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.35416666666666"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# population variance\n",
    "np.hstack((x1, x2, x3)).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185.3402777777778"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance from the paper\n",
    "sum([x1.var(),x2.var(), x3.var()])/3*11/(12-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.083333333333332"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean from the paper\n",
    "sum([x1.mean(),x2.mean(), x3.mean()])/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the moving mean and variance equation:\n",
    "\n",
    "$\\mu{_{mm}}$[B] = momentum*$\\mu{_{mm}}$[B] + (1-momentum)*$\\mu$[B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.507999999999999"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmm = 0\n",
    "test_momntm = 0.9\n",
    "\n",
    "for i in (x1, x2, x3):\n",
    "    testmm = test_momntm*testmm +  (1-test_momntm)*(i.mean())\n",
    "testmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.07429166666665"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testvr = 0\n",
    "test_momntm = 0.9\n",
    "\n",
    "for i in (x1, x2, x3):\n",
    "    testvr = test_momntm*testvr +  (1-test_momntm)*(i.var())\n",
    "testvr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * These are clearly wrong. Its obvious when one realizes that the moving mean initializer is zero.\n",
    "      In searching for the definition, the above equation was returned for many hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing the weights\n",
    "\n",
    "$\\mu{_{mm}}$[B] = (1-momentum)*$\\mu{_{mm}}$[B] + momentum*$\\mu$[B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.472"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmm = 0\n",
    "test_momntm = 0.9\n",
    "\n",
    "for i in (x1, x2, x3):\n",
    "    testmm = (1-test_momntm)*testmm +  test_momntm*(i.mean())\n",
    "testmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188.153625"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testvr = 0\n",
    "test_momntm = 0.9\n",
    "\n",
    "for i in (x1, x2, x3):\n",
    "    testvr = (1-test_momntm)*testvr +  test_momntm*(i.var())\n",
    "testvr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Much closer to the population mean and variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *End Testing*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 3) dtype=float32, numpy=array([[1., 2., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([[1,2,3]], dtype = 'float32')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = tf.Variable(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1.001, 2.001, 3.001]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1.5015001, 3.0015   , 4.5015   ]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x + offset)*3/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "btch = tf.Variable(10, dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.1001    , 0.20009999, 0.3001    ]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x+offset)/tf.cast(btch, offset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0, dtype = 'int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=10>\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    x.assign_add(i)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *End Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom BatchNorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custombn_paper(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Not implementing momentum as defined in the keras BatchNorm layer\n",
    "    \n",
    "    Population mean and variance are calculated as defined in the paper\n",
    "    \n",
    "    This process can only be used when mini-batch size > 1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, stateful, **kwargs):\n",
    "        \n",
    "        super(custombn_paper, self).__init__(**kwargs)\n",
    "        \n",
    "        self.stateful = stateful\n",
    "        # Without setting stateful, the Weights/Variables in the layers cannot be reset\n",
    "        # model.reset_states()\n",
    "        \n",
    "        \n",
    "#         self.batchsize = 60.0\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "#         self.inputshape = input_shape\n",
    "        self.gamma = self.add_weight(name = 'scale', shape = (1, input_shape[-1]), initializer = tf.keras.initializers.ones(),\n",
    "                                    trainable = self.trainable)\n",
    "        self.beta = self.add_weight(name = 'shift', shape = (1,input_shape[-1]), initializer = tf.keras.initializers.zeros(),\n",
    "                                   trainable = self.trainable)\n",
    "        self.offset = tf.Variable(0.001, dtype = 'float32', trainable=False)\n",
    "        \n",
    "        self.moving_mean = self.add_weight(name = 'moving_mean', shape = (1, input_shape[-1]), initializer = tf.keras.initializers.Zeros(),\n",
    "                                          trainable = False)\n",
    "        self.moving_var =  self.add_weight(name = 'moving_var', shape = (1, input_shape[-1]), initializer = tf.keras.initializers.Zeros(),\n",
    "                                          trainable = False)\n",
    "        \n",
    "        self.batch_count = tf.Variable(0, dtype = 'float32', name = 'batchcount', trainable=False)\n",
    "        \n",
    "        self.batchsize = tf.Variable(2, dtype = 'float32', name='batchsize', trainable=False)\n",
    "\n",
    "        \n",
    "        self.init_mm = self.moving_mean.read_value()\n",
    "        self.init_mv = self.moving_var.read_value()\n",
    "#         self.init_bc = self.batch_count.read_value()\n",
    "#         self.init_bs = self.batchsize.read_value()\n",
    "    \n",
    "    def bn_training(self, inputs, axes = [0]):\n",
    "        \n",
    "#         self.batchsize = inputs.shape[0]\n",
    "        \n",
    "        \n",
    "        tf.print('\\nBatch size: ', self.batchsize)\n",
    "        self.batch_mean, self.batch_var = tf.nn.moments(inputs, axes = axes, keepdims=True)\n",
    "        \n",
    "        self.moving_mean.assign_add(self.batch_mean)\n",
    "        self.moving_var.assign_add(self.batch_var)\n",
    "\n",
    "\n",
    "        # self.moving_var = self.moving_var + self.batch_var\n",
    "        # doing this causes the object to transform into a constant tensor rather than a Variable or weight\n",
    "       \n",
    "        \n",
    "        return tf.add(tf.multiply(tf.divide(tf.subtract(inputs, self.batch_mean), \n",
    "                                     tf.math.sqrt(tf.add(self.batch_var, self.offset))), self.gamma), self.beta)\n",
    "    \n",
    "\n",
    "    \n",
    "    def update_mm_mv(self):\n",
    "        \"\"\"\n",
    "        Updating mm and mv at the end of epoch\n",
    "        \"\"\"\n",
    "        tf.print(\"\\nIn update_mm_mv function\")\n",
    "        \n",
    "        # self.batchsize = tf.cast(self.batchsize, tf.float32)\n",
    "        # This is becoming None at the end of the last batch during training through bn_training\n",
    "        # found by executing on_train_batch_end. This implies that bn_training is executed upon\n",
    "        # execution of the last training batch. But why?\n",
    "\n",
    "        \n",
    "        self.moving_mean.assign(tf.cond(tf.greater(self.batch_count,0), \n",
    "                                        lambda: tf.divide(self.moving_mean,self.batch_count), lambda: self.moving_mean, name='update_mm'))\n",
    "        \n",
    "        self.moving_var.assign(tf.cond(tf.greater(self.batch_count,0), \n",
    "                                       lambda: tf.multiply(self.moving_var, tf.divide(self.batchsize, tf.multiply(tf.subtract(self.batchsize,1), self.batch_count))),\n",
    "                                       lambda: self.moving_var, name='update_mv'))\n",
    "        \n",
    "    \n",
    "    def bn_inference(self, inputs):\n",
    "        \n",
    "\n",
    "        # if self.batchsize == None:\n",
    "        #     self.batchsize = 2    \n",
    "        # This is required because during building the model, this function is also being evaluated\n",
    "        # and giving \"TypeError: unsupported operand type(s) for -: 'NoneType' and 'int' \" because batchsize is None\n",
    "#         @tf.function\n",
    "#         def testprint(inpt):\n",
    "#             tf.print(inpt)\n",
    "              \n",
    "        tf.print(\"\\n I am in inference mode \\n\")\n",
    "        tf.print(\"Moving mean: \", self.moving_mean[0,:5])\n",
    "        \n",
    "        return tf.add(tf.multiply(tf.divide(tf.subtract(inputs, self.moving_mean), \n",
    "                                     tf.math.sqrt(tf.add(self.moving_var, self.offset))), self.gamma), self.beta)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset_states(self):\n",
    "      \n",
    "        # self.batch_count.assign(self.init_bc)\n",
    "        self.moving_mean.assign(self.init_mm)\n",
    "        self.moving_var.assign(self.init_mv)\n",
    "        \n",
    "\n",
    "#     def reset_running_values(self):\n",
    "#         # input_shape = self.get_input_shape_at(0)\n",
    "#         # Does work under Eager mode\n",
    "\n",
    "#         self.moving_mean.assign(self.init_mm)\n",
    "#         self.moving_var.assign(self.init_mv)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training):       \n",
    "        \n",
    "        return tf.cond(tf.equal(training, True, name='train_or_eval'),lambda: self.bn_training(inputs), lambda: self.bn_inference(inputs),\n",
    "                      name = 'call_func') \n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(custombn_paper, self).get_config()\n",
    "        config.update({'stateful': self.stateful})\n",
    "        \n",
    "        return config\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=uint8, numpy=0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(False, tf.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=False>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = False\n",
    "tf.equal(x, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = x.read_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.assign(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.assign(temp)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.resource_variable_ops.ResourceVariable"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = None\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y) == 'NoneType'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *End Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph():\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0., 1., 2., 3., 4.],\n",
       "       [5., 6., 7., 8., 9.]], dtype=float32)>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.reshape(tf.range(0,10, dtype='float32'), shape=(2,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bnpaper = custombn_paper(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[-0.99992, -0.99992, -0.99992, -0.99992, -0.99992],\n",
       "       [ 0.99992,  0.99992,  0.99992,  0.99992,  0.99992]], dtype=float32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnpaper(x, training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'custombn_paper/scale:0' shape=(1, 5) dtype=float32, numpy=array([[1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/shift:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=0.001>,\n",
       " <tf.Variable 'custombn_paper/moving_mean:0' shape=(1, 5) dtype=float32, numpy=array([[2.5, 3.5, 4.5, 5.5, 6.5]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/moving_var:0' shape=(1, 5) dtype=float32, numpy=array([[6.25, 6.25, 6.25, 6.25, 6.25]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=1.0>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=2.0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnpaper.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bnpaper.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'custombn_paper/scale:0' shape=(1, 5) dtype=float32, numpy=array([[1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/shift:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=0.001>,\n",
       " <tf.Variable 'custombn_paper/moving_mean:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/moving_var:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=2.0>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnpaper.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bnpaper.training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[  0.      ,  31.622774,  63.24555 ,  94.868324, 126.4911  ],\n",
       "       [158.11388 , 189.73665 , 221.35942 , 252.9822  , 284.60498 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnpaper(x, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[  0.      ,  31.622774,  63.24555 ,  94.868324, 126.4911  ],\n",
       "       [158.11388 , 189.73665 , 221.35942 , 252.9822  , 284.60498 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.add(tf.multiply(tf.divide(tf.subtract(x, test_bnpaper.moving_mean), \n",
    "                                     tf.math.sqrt(tf.add(test_bnpaper.moving_var, test_bnpaper.offset))), test_bnpaper.gamma), test_bnpaper.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Confirms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'custombn_paper/scale:0' shape=(1, 5) dtype=float32, numpy=array([[1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/shift:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=0.001>,\n",
       " <tf.Variable 'custombn_paper/moving_mean:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/moving_var:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'custombn_paper/Variable:0' shape=() dtype=float32, numpy=2.0>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnpaper.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bnpaper.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * custom defined same as keras defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fitting_bnpaper(inputshape = [1], units1 = 100, units2 =100, units3=100, classes=10):\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=None, name = 'input')\n",
    "    \n",
    "    dense1 = dense_layer(units = units1, name = 'dense1', trainable = True)(input_lyr)\n",
    "    custombn1 = custombn_paper(stateful = True, name = 'bn1', trainable = True)(dense1)     \n",
    "    activation1 = tf.keras.layers.Activation(activation = tf.nn.tanh, name='actv1')(custombn1)\n",
    "    \n",
    "    dense2 = dense_layer(units = units2, name = 'dense2', trainable = True)(activation1)    \n",
    "    custombn2 = custombn_paper(stateful = True, name = 'bn2', trainable = True)(dense2)     \n",
    "    activation2 = tf.keras.layers.Activation(activation = tf.nn.tanh, name='actv2')(custombn2)\n",
    "    \n",
    "    dense3 = dense_layer(units = units3, name = 'dense3', trainable = True)(activation2)    \n",
    "    custombn3 = custombn_paper(stateful = True, name='bn3', trainable = True)(dense3)     \n",
    "    activation3 = tf.keras.layers.Activation(activation = tf.nn.tanh, name='actv3')(custombn3)\n",
    "    \n",
    "    \n",
    "    output_lyr = dense_layer(units = classes, name='output') (activation3)\n",
    "    # no softmax \n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (785,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel_hAPI = model_fitting_bnpaper(inputshape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in testmodel_hAPI.layers:\n",
    "    \n",
    "    if hasattr(layer, 'call'):\n",
    "        if hasattr(layer.call, 'training'):\n",
    "            print(layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.custombn_paper object at 0x7fdaac3fcc88>\n",
      "<__main__.custombn_paper object at 0x7fdaac363828>\n",
      "<__main__.custombn_paper object at 0x7fdaac348a90>\n"
     ]
    }
   ],
   "source": [
    "for layer in testmodel_hAPI.layers:\n",
    "\n",
    "    if getattr(layer, 'stateful', False):\n",
    "        print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "custombn_paper (custombn_pap (None, 100)               403       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "custombn_paper_1 (custombn_p (None, 100)               403       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense3 (dense_layer)         (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "custombn_paper_2 (custombn_p (None, 100)               403       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_layer (dense_layer)    (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 100,919\n",
      "Trainable params: 100,310\n",
      "Non-trainable params: 609\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "testmodel_hAPI.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataset.take(1):\n",
    "    x = i[0][0:2]\n",
    "    y = i[1][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 784])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training = True\n",
    "output1 =  testmodel_hAPI(x, training =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.07456852,  0.3217428 ,  0.76883996, -0.56115365,  0.5446787 ,\n",
       "        -0.7092006 , -0.08767772, -0.36047605, -1.1830816 , -1.3314551 ],\n",
       "       [-0.07456852, -0.3217428 , -0.76883996,  0.56115365, -0.5446787 ,\n",
       "         0.7092006 ,  0.08767772,  0.36047605,  1.1830816 ,  1.3314551 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = testmodel_hAPI.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'custombn_paper/moving_mean:0' shape=(1, 100) dtype=float32, numpy=\n",
       "array([[-0.21910067, -0.04545818, -0.36881888, -0.35995263, -0.37876043,\n",
       "         0.5636653 , -0.45316696,  0.41576093,  0.06434174, -0.13719991,\n",
       "         0.7114807 ,  0.16890293, -0.26944566,  0.70864797,  1.1258295 ,\n",
       "        -0.18561685, -0.47316995, -0.48458487, -0.47439256,  0.26698315,\n",
       "        -0.06854546, -0.32742986, -0.7525749 ,  1.3198287 ,  0.04259807,\n",
       "        -0.03536795,  0.6654351 , -0.3738553 ,  0.6054493 ,  0.90603566,\n",
       "         0.76952225,  0.08472401, -0.6430973 ,  0.4030885 ,  0.85232085,\n",
       "        -0.151348  ,  0.60588497, -0.08739981,  0.14842562, -0.11197841,\n",
       "         0.08410388, -0.67636263,  1.1705109 , -0.26388553,  1.1846156 ,\n",
       "        -0.1185993 ,  0.03499955,  0.5700469 , -1.1087158 ,  1.2339005 ,\n",
       "         0.07632702, -0.5952873 , -0.17209323, -0.5292679 ,  0.5420853 ,\n",
       "        -0.21385705,  1.1709832 ,  0.8948821 ,  0.05686894,  0.10488322,\n",
       "        -0.5455679 , -0.01418075,  0.91083986, -0.1661529 ,  0.4494189 ,\n",
       "        -0.2769004 ,  0.35358614,  0.6117253 , -0.04027766,  0.99596155,\n",
       "         0.05259049,  0.08331385,  0.29931456,  0.5620724 , -0.46190235,\n",
       "         0.2046018 ,  0.81884545, -0.17251576,  0.10001938, -0.2019344 ,\n",
       "        -0.2712328 ,  0.2931524 ,  0.03829199, -0.7677891 ,  0.01530267,\n",
       "        -1.0840274 ,  0.559898  ,  0.25879392, -1.043683  ,  0.5182303 ,\n",
       "         0.00721753, -0.12236457, -0.6223352 ,  0.08871326, -0.12139866,\n",
       "        -0.43421662, -0.04812601, -0.6966479 ,  0.68212366, -0.17084914]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'custombn_paper/moving_var:0' shape=(1, 100) dtype=float32, numpy=\n",
       "array([[1.11323716e-04, 8.17310363e-02, 1.31485581e-01, 1.64697424e-01,\n",
       "        2.33810209e-02, 5.14779016e-02, 8.58729109e-02, 1.08998859e+00,\n",
       "        3.23619954e-02, 4.93912309e-01, 9.71609354e-02, 1.28200173e-01,\n",
       "        1.63687199e-01, 1.36766046e-01, 4.55434084e-01, 1.05486429e+00,\n",
       "        1.49152905e-01, 9.58062485e-02, 9.18560982e-01, 2.37562522e-01,\n",
       "        2.97709733e-01, 4.52963471e-01, 1.09890640e+00, 1.49301499e-01,\n",
       "        4.33003455e-01, 4.59127419e-04, 1.29009755e-02, 6.23052620e-05,\n",
       "        8.26743767e-02, 5.22027444e-03, 2.05745831e-01, 1.74092129e-01,\n",
       "        9.75580215e-01, 9.36574787e-02, 1.11721694e+00, 5.12747467e-01,\n",
       "        3.41698825e-01, 4.75322604e-02, 1.15777701e-01, 4.22812760e-01,\n",
       "        8.27344894e-01, 6.81075752e-01, 1.96245368e-04, 1.25495404e-01,\n",
       "        1.67617381e-01, 7.79511333e-01, 5.90005307e-04, 4.40345937e-03,\n",
       "        9.27318633e-03, 3.53955209e-01, 4.33561280e-02, 5.69901541e-02,\n",
       "        2.44328782e-01, 2.32479930e-01, 1.31871745e-01, 2.27629337e-02,\n",
       "        1.18386269e-01, 2.10457850e+00, 1.23165756e-01, 3.15649360e-02,\n",
       "        9.62111056e-02, 9.35856476e-02, 7.60324359e-01, 4.28015649e-01,\n",
       "        1.29364714e-01, 5.31040609e-01, 7.98764169e-01, 8.18203110e-03,\n",
       "        1.68678296e+00, 4.53640878e-01, 1.43287933e+00, 1.50444746e-01,\n",
       "        4.09377456e-01, 2.25593708e-02, 8.57988596e-02, 1.32212730e-03,\n",
       "        6.46336004e-02, 8.65100175e-02, 6.32264167e-02, 6.09415948e-01,\n",
       "        2.08878130e-01, 7.33837485e-02, 1.32998002e+00, 3.32774639e-01,\n",
       "        1.65899739e-01, 1.22618127e+00, 8.24599266e-01, 3.32954615e-01,\n",
       "        1.69838648e-02, 2.12788349e-03, 2.90359650e-02, 7.51162097e-02,\n",
       "        4.12259012e-01, 5.10890961e-01, 3.62867743e-01, 9.72143471e-01,\n",
       "        8.49635154e-02, 1.63022906e-01, 8.74312937e-01, 4.02396977e-01]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel_hAPI.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'custombn_paper/moving_mean:0' shape=(1, 100) dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'custombn_paper/moving_var:0' shape=(1, 100) dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.moving_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "del testmodel_hAPI\n",
    "reset_graph()\n",
    "testmodel_hAPI = model_fitting_bnpaper(inputshape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting training = False\n",
    "output1 =  testmodel_hAPI(x, training =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.74668825,  1.2636757 , -0.08159411,  0.34712332, -0.19115561,\n",
       "         0.2916773 , -0.13474743,  0.03781182, -0.7408727 , -1.3356284 ],\n",
       "       [ 1.8295116 , -0.5051032 , -1.4750142 ,  0.04484307,  0.20471641,\n",
       "         0.24860117, -1.1646487 ,  0.6938643 , -0.349613  ,  1.1323991 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'custombn_paper/moving_mean:0' shape=(1, 100) dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testmodel_hAPI.layers[2].moving_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(testmodel_hAPI.layers[2].moving_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). To check if passing the **training** arg to Model results in it passing the same to all its layers: **Works**\n",
    "\n",
    "2). Model call arguments are **inputs, training and mask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del testmodel_hAPI\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Callback for resetting moving mean and variances at the end of each epoch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_callback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    \"\"\"\n",
    "    This callback resets the moving mean and variances at the end of each epoch    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batchsize):\n",
    "        \n",
    "        super(custom_callback, self).__init__()\n",
    "                \n",
    "        self.batchsize = batchsize\n",
    "        self.batchcount = tf.Variable(0, dtype = tf.int32, trainable=False)\n",
    "        \n",
    "        \n",
    "    def on_train_begin(self, logs =None):\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if layer.__class__.__name__ == 'custombn_paper':\n",
    "                layer.batchsize.assign(self.batchsize)       \n",
    "        \n",
    "          \n",
    "#     @tf.function # This gave a slow execution warning\n",
    "#     def batch_count_update(self, x):\n",
    "#         if x ==1:\n",
    "\n",
    "#             # batchcount = self.model.history.params['steps']\n",
    "#             # Using this object gives \n",
    "#             # \"UnboundLocalError: local variable 'batchcount' referenced before assignment\"\n",
    "#             # when defined in \"elif batch == batchcount\"\n",
    "\n",
    "#             for layer in self.model.layers:\n",
    "#                 if layer.__class__.__name__ == 'custombn_paper':\n",
    "\n",
    "#                     layer.batch_count.assign(self.model.history.params['steps'])\n",
    "#                     tf.print('\\nbatch count is: ', layer.batch_count)\n",
    "\n",
    "#             tf.print(\"\\nEnd of 1st training batch: \", self.model.history.params['steps'])\n",
    "\n",
    "#         elif x == self.model.history.params['steps']-1:\n",
    "\n",
    "#             for layer in self.model.layers:\n",
    "#                 if hasattr(layer, 'update_mm_mv'):\n",
    "#                     tf.print('\\nUpdating mm/mv of ', layer.name)\n",
    "#                     layer.update_mm_mv()\n",
    "    \n",
    "    \n",
    "    def at_batch_one(self):\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            if layer.__class__.__name__ == 'custombn_paper':\n",
    "\n",
    "                layer.batch_count.assign(self.model.history.params['steps'])\n",
    "                tf.print('\\nbatch count is: ', layer.batch_count)\n",
    "\n",
    "        tf.print(\"\\nEnd of 1st training batch: \", self.model.history.params['steps'])\n",
    "        return None\n",
    "            \n",
    "    \n",
    "    def at_last_batch(self):\n",
    "\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'update_mm_mv'):\n",
    "                tf.print('\\nUpdating mm/mv of ', layer.name)\n",
    "                layer.update_mm_mv()\n",
    "        return None\n",
    "    \n",
    "    def at_any_batch(self):\n",
    "        return None\n",
    "    \n",
    "    def at_batch_not_one(self):\n",
    "        tf.cond(tf.equal(self.batchcount, self.model.history.params['steps']-1), self.at_last_batch, self.at_any_batch)\n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.batchcount.assign(batch)\n",
    "        tf.print(\"On train batch end, batch no: \", self.batchcount)\n",
    "        tf.cond(tf.equal(self.batchcount,1), self.at_batch_one, self.at_batch_not_one)     \n",
    "        \n",
    "        \n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        self.model.reset_states()\n",
    "        \n",
    "        tf.print('\\nEpoch begin: moving mean values of the 1st BN layer: ', self.model.layers[2].moving_mean[0,:5])\n",
    "        \n",
    "\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "                    \n",
    "               \n",
    "        tf.print('\\nEpoch End: moving mean values of the 1st BN layer: ', self.model.layers[2].moving_mean[0,:5])\n",
    "        \n",
    "        \n",
    "# customcb = custom_callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Callback for saving best model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bnlayer_model = 'cstm_bnlayer_model'\n",
    "\n",
    "if os.path.exists(custom_bnlayer_model):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(custom_bnlayer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_savemodel = tf.keras.callbacks.ModelCheckpoint(os.path.join(custom_bnlayer_model, 'model_{epoch}-{val_loss:.3f}.h5'), mode = 'min',\\\n",
    "                                                 monitor = 'val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Callback for Visualizing in TensorBoard*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bnlayer_logs = 'cstm_bnlayer_logs'\n",
    "\n",
    "if os.path.exists(custom_bnlayer_logs):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(custom_bnlayer_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstm_tb = tf.keras.callbacks.TensorBoard(log_dir = custom_bnlayer_logs, histogram_freq=1, write_graph=True,\\\n",
    "                                        write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.callbacks.TensorBoard at 0x7fda3d7efc88>,\n",
       " <tensorflow.python.keras.callbacks.ModelCheckpoint at 0x7fd9fdda3d68>,\n",
       " <__main__.custom_callback at 0x7fda3d716f28>]"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_list = []\n",
    "cb_list.append([cstm_tb,cb_savemodel, custom_callback(batchsize=60)]) \n",
    "cb_list = cb_list[0]\n",
    "cb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Adam optimizer has adaptive learning rate feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Model using custom defined BN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model_bnpaper\n",
    "# reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnpaper = model_fitting_bnpaper(inputshape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "bn1 (custombn_paper)         (None, 100)               403       \n",
      "_________________________________________________________________\n",
      "actv1 (Activation)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "bn2 (custombn_paper)         (None, 100)               403       \n",
      "_________________________________________________________________\n",
      "actv2 (Activation)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense3 (dense_layer)         (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "bn3 (custombn_paper)         (None, 100)               403       \n",
      "_________________________________________________________________\n",
      "actv3 (Activation)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (dense_layer)         (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 100,919\n",
      "Trainable params: 100,310\n",
      "Non-trainable params: 609\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bnpaper.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'model',\n",
       " 'layers': [{'class_name': 'InputLayer',\n",
       "   'config': {'batch_input_shape': (None, 784),\n",
       "    'dtype': 'float32',\n",
       "    'sparse': False,\n",
       "    'ragged': False,\n",
       "    'name': 'input'},\n",
       "   'name': 'input',\n",
       "   'inbound_nodes': []},\n",
       "  {'class_name': 'dense_layer',\n",
       "   'config': {'name': 'dense1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 100},\n",
       "   'name': 'dense1',\n",
       "   'inbound_nodes': [[['input', 0, 0, {}]]]},\n",
       "  {'class_name': 'custombn_paper',\n",
       "   'config': {'name': 'bn1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'stateful': True},\n",
       "   'name': 'bn1',\n",
       "   'inbound_nodes': [[['dense1', 0, 0, {}]]]},\n",
       "  {'class_name': 'Activation',\n",
       "   'config': {'name': 'actv1',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'tanh'},\n",
       "   'name': 'actv1',\n",
       "   'inbound_nodes': [[['bn1', 0, 0, {}]]]},\n",
       "  {'class_name': 'dense_layer',\n",
       "   'config': {'name': 'dense2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 100},\n",
       "   'name': 'dense2',\n",
       "   'inbound_nodes': [[['actv1', 0, 0, {}]]]},\n",
       "  {'class_name': 'custombn_paper',\n",
       "   'config': {'name': 'bn2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'stateful': True},\n",
       "   'name': 'bn2',\n",
       "   'inbound_nodes': [[['dense2', 0, 0, {}]]]},\n",
       "  {'class_name': 'Activation',\n",
       "   'config': {'name': 'actv2',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'tanh'},\n",
       "   'name': 'actv2',\n",
       "   'inbound_nodes': [[['bn2', 0, 0, {}]]]},\n",
       "  {'class_name': 'dense_layer',\n",
       "   'config': {'name': 'dense3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 100},\n",
       "   'name': 'dense3',\n",
       "   'inbound_nodes': [[['actv2', 0, 0, {}]]]},\n",
       "  {'class_name': 'custombn_paper',\n",
       "   'config': {'name': 'bn3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'stateful': True},\n",
       "   'name': 'bn3',\n",
       "   'inbound_nodes': [[['dense3', 0, 0, {}]]]},\n",
       "  {'class_name': 'Activation',\n",
       "   'config': {'name': 'actv3',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'activation': 'tanh'},\n",
       "   'name': 'actv3',\n",
       "   'inbound_nodes': [[['bn3', 0, 0, {}]]]},\n",
       "  {'class_name': 'dense_layer',\n",
       "   'config': {'name': 'output',\n",
       "    'trainable': True,\n",
       "    'dtype': 'float32',\n",
       "    'units': 10},\n",
       "   'name': 'output',\n",
       "   'inbound_nodes': [[['actv3', 0, 0, {}]]]}],\n",
       " 'input_layers': [['input', 0, 0]],\n",
       " 'output_layers': [['output', 0, 0]]}"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bnpaper.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bnpaper.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnpaper.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\\\n",
    "                     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics = [tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs1=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps, validate for 50 steps\n",
      "\n",
      "Epoch begin: moving mean values of the 1st BN layer:  [0 0 0 0 0]\n",
      "Epoch 1/2\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  0\n",
      "  1/100 [..............................] - ETA: 2:11 - loss: 2.4268 - categorical_accuracy: 0.1167\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  1\n",
      "\n",
      "batch count is:  100\n",
      "\n",
      "batch count is:  100\n",
      "\n",
      "batch count is:  100\n",
      "\n",
      "End of 1st training batch:  100\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  2\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  3\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  4\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  5\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  6\n",
      "  7/100 [=>............................] - ETA: 18s - loss: 1.7514 - categorical_accuracy: 0.4333 \n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  7\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  8\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  9\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  10\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  11\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  12\n",
      " 13/100 [==>...........................] - ETA: 9s - loss: 1.4492 - categorical_accuracy: 0.5436 \n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  13\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  14\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  15\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  16\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  17\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  18\n",
      " 19/100 [====>.........................] - ETA: 6s - loss: 1.2606 - categorical_accuracy: 0.6140\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  19\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  20\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  21\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  22\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  23\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  24\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  25\n",
      " 26/100 [======>.......................] - ETA: 4s - loss: 1.0932 - categorical_accuracy: 0.6718\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  26\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  27\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  28\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  29\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  30\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  31\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  32\n",
      " 33/100 [========>.....................] - ETA: 3s - loss: 0.9929 - categorical_accuracy: 0.7040\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  33\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  34\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  35\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  36\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  37\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  38\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  39\n",
      " 40/100 [===========>..................] - ETA: 2s - loss: 0.9147 - categorical_accuracy: 0.7308\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  40\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  41\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  42\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  43\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  44\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  45\n",
      " 46/100 [============>.................] - ETA: 2s - loss: 0.8607 - categorical_accuracy: 0.7482\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  46\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  47\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  48\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  49\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  50\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  51\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  52\n",
      " 53/100 [==============>...............] - ETA: 1s - loss: 0.8068 - categorical_accuracy: 0.7629\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  53\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  54\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  55\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  56\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  57\n",
      " 58/100 [================>.............] - ETA: 1s - loss: 0.7770 - categorical_accuracy: 0.7724\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  58\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  59\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  61\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  62\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  63\n",
      " 64/100 [==================>...........] - ETA: 1s - loss: 0.7440 - categorical_accuracy: 0.7826\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  64\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  65\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  66\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  67\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  68\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  69\n",
      " 70/100 [====================>.........] - ETA: 0s - loss: 0.7162 - categorical_accuracy: 0.7921\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  70\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  71\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  72\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  73\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  74\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  75\n",
      " 76/100 [=====================>........] - ETA: 0s - loss: 0.6916 - categorical_accuracy: 0.7991\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  76\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  77\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  78\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  79\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  80\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  81\n",
      " 82/100 [=======================>......] - ETA: 0s - loss: 0.6725 - categorical_accuracy: 0.8049\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  82\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  83\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  84\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  85\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  86\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  87\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  88\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  89\n",
      " 90/100 [==========================>...] - ETA: 0s - loss: 0.6492 - categorical_accuracy: 0.8122\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  90\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  91\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  92\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  93\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  94\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  95\n",
      " 96/100 [===========================>..] - ETA: 0s - loss: 0.6304 - categorical_accuracy: 0.8184\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  96\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  97\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  98\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6237 - categorical_accuracy: 0.8207\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  99\n",
      "\n",
      "Updating mm/mv of  bn1\n",
      "\n",
      "In update_mm_mv function\n",
      "\n",
      "Updating mm/mv of  bn2\n",
      "\n",
      "In update_mm_mv function\n",
      "\n",
      "Updating mm/mv of  bn3\n",
      "\n",
      "In update_mm_mv function\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0248715 0.0237944871 0.0484486595 0.008207337 -0.0135713639]\n",
      "Moving mean:  [-0.0135248387 -0.0110725136 -0.000717982475 -0.0339886807 0.037179336]\n",
      "Moving mean:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "\n",
      "Epoch End: moving mean values of the 1st BN layer:  [-0.00393687375 0.00751831941 0.016441701 0.0149505101 0.0348855741]\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.6207 - categorical_accuracy: 0.8218 - val_loss: 0.3760 - val_categorical_accuracy: 0.8953\n",
      "\n",
      "Epoch begin: moving mean values of the 1st BN layer:  [0 0 0 0 0]\n",
      "Epoch 2/2\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  0\n",
      "  1/100 [..............................] - ETA: 30s - loss: 0.2394 - categorical_accuracy: 0.9500\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  1\n",
      "\n",
      "batch count is:  100\n",
      "\n",
      "batch count is:  100\n",
      "\n",
      "batch count is:  100\n",
      "\n",
      "End of 1st training batch:  100\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  2\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  3\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  4\n",
      "  5/100 [>.............................] - ETA: 6s - loss: 0.3134 - categorical_accuracy: 0.9233 \n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  5\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  6\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  7\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  8\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  9\n",
      " 10/100 [==>...........................] - ETA: 3s - loss: 0.3179 - categorical_accuracy: 0.9167\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  10\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  11\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  12\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  13\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  14\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  15\n",
      " 16/100 [===>..........................] - ETA: 2s - loss: 0.3396 - categorical_accuracy: 0.9073\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  16\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  17\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  18\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  19\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  20\n",
      " 21/100 [=====>........................] - ETA: 1s - loss: 0.3355 - categorical_accuracy: 0.9071\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  21\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  22\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  23\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  24\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  25\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  26\n",
      " 27/100 [=======>......................] - ETA: 1s - loss: 0.3311 - categorical_accuracy: 0.9093\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  27\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  28\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  29\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  30\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  31\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  32\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  33\n",
      " 34/100 [=========>....................] - ETA: 1s - loss: 0.3305 - categorical_accuracy: 0.9064\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  34\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  35\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  36\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  37\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  38\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  39\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  40\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  41\n",
      " 42/100 [===========>..................] - ETA: 0s - loss: 0.3302 - categorical_accuracy: 0.9048\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  42\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  43\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  44\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  45\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  46\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  47\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  48\n",
      " 49/100 [=============>................] - ETA: 0s - loss: 0.3343 - categorical_accuracy: 0.9044\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  49\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  50\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  51\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  52\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  53\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  54\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  55\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  56\n",
      " 57/100 [================>.............] - ETA: 0s - loss: 0.3389 - categorical_accuracy: 0.9041\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  57\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  58\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  59\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  61\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  62\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  63\n",
      " 64/100 [==================>...........] - ETA: 0s - loss: 0.3345 - categorical_accuracy: 0.9034\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  64\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  65\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  66\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  67\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  68\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  69\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  70\n",
      " 71/100 [====================>.........] - ETA: 0s - loss: 0.3359 - categorical_accuracy: 0.9019\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  71\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  72\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  73\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  74\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  75\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  76\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  77\n",
      " 78/100 [======================>.......] - ETA: 0s - loss: 0.3361 - categorical_accuracy: 0.9017\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  78\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  79\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  80\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  81\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  82\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  83\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  84\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  85\n",
      " 86/100 [========================>.....] - ETA: 0s - loss: 0.3351 - categorical_accuracy: 0.9016\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  86\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  87\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  88\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  89\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  90\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  91\n",
      " 92/100 [==========================>...] - ETA: 0s - loss: 0.3337 - categorical_accuracy: 0.9011\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  92\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  93\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  94\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  95\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  96\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  97\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.3337 - categorical_accuracy: 0.9014\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  98\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "\n",
      "Batch size:  60\n",
      "On train batch end, batch no:  99\n",
      "\n",
      "Updating mm/mv of  bn1\n",
      "\n",
      "In update_mm_mv function\n",
      "\n",
      "Updating mm/mv of  bn2\n",
      "\n",
      "In update_mm_mv function\n",
      "\n",
      "Updating mm/mv of  bn3\n",
      "\n",
      "In update_mm_mv function\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "\n",
      " I am in inference mode \n",
      "\n",
      "Moving mean:  [-0.0330172032 0.0437782817 0.0576309785 0.0156156449 -0.0136411665]\n",
      "Moving mean:  [-0.0209568162 -0.00027273767 -8.91988238e-05 -0.0262907688 0.0398673639]\n",
      "Moving mean:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "\n",
      "Epoch End: moving mean values of the 1st BN layer:  [0.00444425968 -0.00348836835 -0.0157718342 -0.00906044617 0.00202425709]\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.3329 - categorical_accuracy: 0.9015 - val_loss: 0.3199 - val_categorical_accuracy: 0.9087\n"
     ]
    }
   ],
   "source": [
    "cstm_bnlayer_history1 =  model_bnpaper.fit(train_dataset.take(100), epochs=epochs1, verbose=1, \n",
    "                                           callbacks=cb_list, validation_data=valid_dataset.take(50), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00444, -0.00349, -0.01577, -0.00906,  0.00202], dtype=float32)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bnpaper.layers[2].moving_mean[0,:5].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.88438, 2.72641, 2.07334, 1.61601, 2.68136], dtype=float32)"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bnpaper.layers[2].moving_var[0,:5].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'bn1/batchcount:0' shape=() dtype=float32, numpy=100.0>"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bnpaper.layers[2].batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Why isn't on_epoch_begin() not allowing to reset the values?\n",
    "    Ans: Because the Variables should be updated by using assign method, and not by direct assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_bnpaper\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custombn_keras(tf.keras.layers.Layer):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implementing momentum as defined in the keras BatchNorm layer to calculate\n",
    "    \n",
    "    Population mean and variance \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, momentum, stateful, **kwargs):\n",
    "        \n",
    "        super(custombn_keras, self).__init__(**kwargs)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "        self.stateful = stateful\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.inputshape = input_shape\n",
    "        self.gamma = self.add_weight(name = 'scale', shape = (1, input_shape[-1]), initializer = tf.keras.initializers.ones(),\n",
    "                                    trainable = self.trainable)\n",
    "        self.beta = self.add_weight(name = 'shift', shape = (1,input_shape[-1]), initializer = tf.keras.initializers.zeros(),\n",
    "                                   trainable = self.trainable)\n",
    "        self.offset = tf.Variable(0.001, dtype = 'float32', trainable=False)\n",
    "        \n",
    "        self.moving_mean = self.add_weight(name = 'moving_mean', shape = (1, input_shape[-1]), initializer = tf.keras.initializers.Zeros(),\n",
    "                                          trainable = False)\n",
    "        self.moving_var =  self.add_weight(name = 'moving_var', shape = (1, input_shape[-1]), initializer = tf.keras.initializers.Zeros(),\n",
    "                                          trainable = False)\n",
    "        \n",
    "        self.init_mm = self.moving_mean.read_value()\n",
    "        self.init_mv = self.moving_var.read_value()\n",
    "        \n",
    "    \n",
    "    def bn_training(self, inputs, axes = [0]):\n",
    "        \n",
    "        \n",
    "        self.batch_mean, self.batch_var = tf.nn.moments(inputs, axes = axes, keepdims=True)\n",
    "        \n",
    "        self.moving_mean.assign((1-self.momentum)*self.moving_mean + self.momentum*self.batch_mean)\n",
    "        self.moving_var.assign((1-self.momentum)*self.moving_var + self.momentum*self.batch_var)\n",
    "        \n",
    "        \n",
    "        return tf.add(tf.multiply(tf.divide(tf.subtract(inputs, self.batch_mean), \n",
    "                                     tf.math.sqrt(tf.add(self.batch_var, self.offset))), self.gamma), self.beta)\n",
    "    \n",
    "    def bn_inference(self, inputs):\n",
    "        \n",
    "        \n",
    "        return tf.add(tf.multiply(tf.divide(tf.subtract(inputs, self.moving_mean), \n",
    "                                     tf.math.sqrt(tf.add(self.moving_var, self.offset))), self.gamma), self.beta)\n",
    "        \n",
    "        \n",
    "    def reset_states(self):\n",
    "        \n",
    "        self.moving_mean.assign(self.init_mm)\n",
    "        self.moving_var.assign(self.init_mv)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training):\n",
    "     \n",
    "        \n",
    "        return tf.cond(tf.equal(training, True),lambda: self.bn_training(inputs), lambda: self.bn_inference(inputs))       \n",
    "    \n",
    "    def get_config(self):\n",
    "        \n",
    "        config = super(custombn_keras, self).get_config()\n",
    "        config.update({'momentum': self.momentum})\n",
    "        \n",
    "        return config\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[0., 1., 2., 3., 4.],\n",
       "       [5., 6., 7., 8., 9.]], dtype=float32)>"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.reshape(tf.range(0,10, dtype = tf.float32), (2,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bnkeras = custombn_keras(momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[-0.99992, -0.99992, -0.99992, -0.99992, -0.99992],\n",
       "       [ 0.99992,  0.99992,  0.99992,  0.99992,  0.99992]], dtype=float32)>"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnkeras(x, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bnkeras.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'custombn_keras_2/scale:0' shape=(1, 5) dtype=float32, numpy=array([[1., 1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_keras_2/shift:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_keras_2/Variable:0' shape=() dtype=float32, numpy=0.001>,\n",
       " <tf.Variable 'custombn_keras_2/moving_mean:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'custombn_keras_2/moving_var:0' shape=(1, 5) dtype=float32, numpy=array([[0., 0., 0., 0., 0.]], dtype=float32)>]"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnkeras.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
       "array([[  0.     ,  31.62277,  63.24555,  94.86832, 126.4911 ],\n",
       "       [158.11388, 189.73665, 221.35942, 252.9822 , 284.60498]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bnkeras(x, training =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *End Testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model function using custombn_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_custombn_keras(inputshape = [1], units1 = 100, units2 =100, units3=100, classes=10):\n",
    "    \n",
    "    input_lyr = tf.keras.Input(shape = inputshape, batch_size=None, name = 'input')\n",
    "    \n",
    "    dense1 = dense_layer(units = units1, name = 'dense1', trainable = True)(input_lyr)\n",
    "    custombn1 = custombn_keras(momentum = 0.99, stateful = True, name = 'bn1', trainable = True)(dense1)     \n",
    "    activation1 = tf.keras.layers.Activation(activation = tf.nn.tanh, name='actv1')(custombn1)\n",
    "    \n",
    "    dense2 = dense_layer(units = units2, name = 'dense2', trainable = True)(activation1)    \n",
    "    custombn2 = custombn_keras(momentum = 0.99, stateful = True, name = 'bn2', trainable = True)(dense2)     \n",
    "    activation2 = tf.keras.layers.Activation(activation = tf.nn.tanh, name='actv2')(custombn2)\n",
    "    \n",
    "    dense3 = dense_layer(units = units3, name = 'dense3', trainable = True)(activation2)    \n",
    "    custombn3 = custombn_keras(momentum = 0.99, stateful = True, name='bn3', trainable = True)(dense3)     \n",
    "    activation3 = tf.keras.layers.Activation(activation = tf.nn.tanh, name='actv3')(custombn3)\n",
    "    \n",
    "    \n",
    "    output_lyr = dense_layer(units = classes, name='output') (activation3)\n",
    "    # no softmax \n",
    "    \n",
    "    return tf.keras.Model(inputs = [input_lyr], outputs = [output_lyr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbn_keras = model_custombn_keras(inputshape=(784,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense1 (dense_layer)         (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "bn1 (custombn_keras)         (None, 100)               401       \n",
      "_________________________________________________________________\n",
      "actv1 (Activation)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense2 (dense_layer)         (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "bn2 (custombn_keras)         (None, 100)               401       \n",
      "_________________________________________________________________\n",
      "actv2 (Activation)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense3 (dense_layer)         (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "bn3 (custombn_keras)         (None, 100)               401       \n",
      "_________________________________________________________________\n",
      "actv3 (Activation)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "output (dense_layer)         (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 100,913\n",
      "Trainable params: 100,310\n",
      "Non-trainable params: 603\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelbn_keras.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for layer in modelbn_keras.layers:\n",
    "    print(layer.stateful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelbn_keras.stateful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custombn_keras_callbck(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        \n",
    "        super(custombn_keras_callbck, self).__init__(**kwargs)\n",
    "            \n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        \n",
    "        self.model.reset_states()\n",
    "        \n",
    "        tf.print(\"\\nMoving mean values are: \", self.model.layers[2].moving_mean[0,:5])\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        \n",
    "        tf.print(\"\\nMoving mean values are: \", self.model.layers[2].moving_mean[0,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bnkeras_svdmodel = 'cstm_bnkeras_svdmodel'\n",
    "\n",
    "if os.path.exists(custom_bnkeras_svdmodel):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(custom_bnkeras_svdmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_savemodel = tf.keras.callbacks.ModelCheckpoint(os.path.join(custom_bnkeras_svdmodel, 'model_{epoch}-{val_loss:.3f}.h5'), mode = 'min',\\\n",
    "                                                 monitor = 'val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bnkeras_logs = 'cstm_bnkeras_logs'\n",
    "\n",
    "if os.path.exists(custom_bnkeras_logs):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(custom_bnkeras_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstm_tb = tf.keras.callbacks.TensorBoard(log_dir = custom_bnkeras_logs, histogram_freq=1, write_graph=True,\\\n",
    "                                        write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.callbacks.TensorBoard at 0x7fd9edd88e48>,\n",
       " <tensorflow.python.keras.callbacks.ModelCheckpoint at 0x7fd9edd88908>,\n",
       " <__main__.custombn_keras_callbck at 0x7fd9edc95080>]"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_list = []\n",
    "cb_list.append([cstm_tb,cb_savemodel, custombn_keras_callbck()]) \n",
    "cb_list = cb_list[0]\n",
    "cb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelbn_keras.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                     loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                     metrics = [tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs1=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps, validate for 50 steps\n",
      "\n",
      "Moving mean values are:  [0 0 0 0 0]\n",
      "Epoch 1/2\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.6182 - categorical_accuracy: 0.8247\n",
      "Moving mean values are:  [0.140929729 -0.0131857274 0.385199726 0.0185528621 -0.202826932]\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.6139 - categorical_accuracy: 0.8262 - val_loss: 0.4121 - val_categorical_accuracy: 0.8713\n",
      "\n",
      "Moving mean values are:  [0 0 0 0 0]\n",
      "Epoch 2/2\n",
      " 89/100 [=========================>....] - ETA: 0s - loss: 0.3175 - categorical_accuracy: 0.9099\n",
      "Moving mean values are:  [-0.0270440895 0.212600231 -0.104504466 -0.132071093 0.0136035718]\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.3188 - categorical_accuracy: 0.9090 - val_loss: 0.3290 - val_categorical_accuracy: 0.9087\n"
     ]
    }
   ],
   "source": [
    "custombn_keras_history1 =  modelbn_keras.fit(train_dataset.take(100), epochs=epochs1, verbose=1, \n",
    "                                           callbacks=cb_list, validation_data=valid_dataset.take(50), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "del modelbn_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Custom code for calculating Batch Normalization written in tensorflow and executed in Keras Layers.\n",
    "\n",
    "> 2. Model with dense and custom batch norm layers run on MNIST dataset. \n",
    "\n",
    "> 3. \\> 90% training and validation accuracy at the end of 2 epochs.\n",
    "\n",
    "> 4. Results cross-checked by replacing the custom batch norm calculation with the tf.nn.batchnormalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
