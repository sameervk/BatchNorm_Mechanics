{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Batch Normalization method\n",
    "\n",
    "    * From the paper \"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\" by\n",
    "      Sergey Ioffe and Christian Szegedy\n",
    "      \n",
    "    * Coding the method using custom layers in tensorflow 2.1.0 to understand the math and comparing against the API\n",
    "    \n",
    "    * Using MNIST data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'2.1.0'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(60000, 28, 28) (10000, 28, 28)\n"
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking min and max values of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "255"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling the data using the Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler = StandardScaler(with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdscaler_fit = stdscaler.fit(x_train.reshape(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(784,)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdscaler_fit.mean_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5.772211140440891"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average mean\n",
    "np.sqrt(stdscaler_fit.mean_.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdscaler_fit.n_samples_seen_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(784,)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stdscaler_fit.scale_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "66.12879201995706"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average std\n",
    "np.sqrt(stdscaler_fit.var_.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 28, 28)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled =  stdscaler_fit.transform(x_train.reshape(-1,28*28)).reshape(-1,28,28)\n",
    "x_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 28, 28)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_scaled =  stdscaler_fit.transform(x_test.reshape(-1,28*28)).reshape(-1,28,28)\n",
    "x_test_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the min and max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-1.2742078920822268"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "244.94693302873063"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9145408163265558"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "-2.1974863349995617e-18"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding the y labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949]))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * The labels are more or less balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8),\n array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009]))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 10)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_coded =  tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_train_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Label:  5 \n One-hot encoded:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
    }
   ],
   "source": [
    "print(\"Label: \", y_train[0],'\\n',\"One-hot encoded: \", y_train_coded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(10000, 10)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_coded =  tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "y_test_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random_seed = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 28, 28)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 10)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled, y_train_coded = shuffle(x_train_scaled, y_train_coded, random_state = random_seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 28, 28)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(60000, 10)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_scaled = x_train_scaled[:5000]\n",
    "y_valid_coded = y_train_coded[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5000, 28, 28)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(5000, 10)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_coded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_train_scaled[5000:]\n",
    "y_train_coded = y_train_coded[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a tf dataset for training on a Model with dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_scaled.reshape(-1, 784), y_train_coded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid_scaled.reshape(-1, 784), y_valid_coded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test_scaled.reshape(-1,784), y_test_coded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch =  60 # in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_size = len(y_train)\n",
    "buffer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle first, batch 2nd, then prefetch\n",
    "train_dataset = train_dataset.shuffle(buffer_size=buffer_size, seed=random_seed, \n",
    "                                      reshuffle_each_iteration=True).batch(batch_size=minibatch, \n",
    "                                                                           drop_remainder=True).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = valid_dataset.shuffle(buffer_size=buffer_size, seed=random_seed, \n",
    "                                      reshuffle_each_iteration=False).batch(batch_size=minibatch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.shuffle(buffer_size=buffer_size, seed=random_seed, \n",
    "                                      reshuffle_each_iteration=False).batch(batch_size=minibatch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(784,) \n\ntf.Tensor([0. 1. 0. 0. 0. 0. 0. 0. 0. 0.], shape=(10,), dtype=float32)\n"
    }
   ],
   "source": [
    "for i in test_dataset.take(1):\n",
    "\n",
    "    print(i[0][0].shape, '\\n')\n",
    "    print(i[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model using Dense Layers and Batch Norm from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * Using the model architecture from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape = (784,), name = 'input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "units1, units2, units3 = 100,100,100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasdense1 = tf.keras.layers.Dense(units = units1, activation=None, \n",
    "                                    kernel_initializer=tf.keras.initializers.Orthogonal(gain=1,seed=random_seed))(input_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and variance are calculated for the minibatch.\n",
    "# setting momentum = 0 for now. Population statistics will be calculated separately for inference.\n",
    "# Initializing moving mean and variance using the statistics from the StandardScaler fit\n",
    "\n",
    "# \"\"\"bn_layer = tf.keras.layers.BatchNormalization(axis = [-1], momentum=0.0, epsilon=0.001, center=True, scale=True, \n",
    "#                                              beta_initializer = tf.keras.initializers.zeros(), \n",
    "#                                               gamma_initializer = tf.keras.initializers.ones(), \n",
    "#                                              moving_mean_initializer = tf.keras.initializers.zeros(), \n",
    "#                                              moving_variance_initializer = tf.keras.initializers.ones(), trainable=True)\"\"\"\n",
    "# the \"bn_layer\" object can be used only once. Have to write a new batch_norm for every layer. Hence, will use a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_layer(axis = [-1]):\n",
    "    \n",
    "    \"\"\"\n",
    "    returns a batch norm layer\n",
    "    \n",
    "    Parameters:\n",
    "    axis: list of integers. Default is [-1] which is confusing because the mean and averages are\n",
    "    calculated across the minibatch rather than the features.   \n",
    "    \n",
    "    Anyways, will be cross-checking against custom code later\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return tf.keras.layers.BatchNormalization(axis = axis, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                             beta_initializer = tf.keras.initializers.zeros(), \n",
    "                                              gamma_initializer = tf.keras.initializers.ones(), \n",
    "                                             moving_mean_initializer = tf.keras.initializers.zeros(), \n",
    "                                             moving_variance_initializer = tf.keras.initializers.ones(), trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasdensebn1 = bn_layer(axis = [-1])(kerasdense1, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation1 = tf.keras.layers.Activation(activation = tf.nn.tanh)(kerasdensebn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasdense2 = tf.keras.layers.Dense(units = units2, activation=None, \n",
    "                                    kernel_initializer=tf.keras.initializers.Orthogonal(gain=1,seed=random_seed))(activation1)\n",
    "kerasdensebn2 = bn_layer(axis = [-1])(kerasdense2, training=True)\n",
    "activation2 = tf.keras.layers.Activation(activation = tf.nn.tanh)(kerasdensebn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasdense3 = tf.keras.layers.Dense(units = units3, activation=None, \n",
    "                                    kernel_initializer=tf.keras.initializers.Orthogonal(gain=1,seed=random_seed))(activation2)\n",
    "kerasdensebn3 = bn_layer(axis = [-1])(kerasdense3, training=True)\n",
    "activation3 = tf.keras.layers.Activation(activation = tf.nn.tanh)(kerasdensebn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(units = 10, activation=None,\n",
    "                                    kernel_initializer=tf.keras.initializers.Orthogonal(gain=1,seed=random_seed))(activation3)\n",
    "# no softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_fc_model = tf.keras.Model(inputs = [input_layer], outputs = [output_layer], name = 'kerasmodel1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"kerasmodel1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput (InputLayer)           [(None, 784)]             0         \n_________________________________________________________________\ndense (Dense)                (None, 100)               78500     \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 100)               400       \n_________________________________________________________________\nactivation (Activation)      (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               10100     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 100)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 100)               10100     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 100)               400       \n_________________________________________________________________\nactivation_2 (Activation)    (None, 100)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 100,910\nTrainable params: 100,310\nNon-trainable params: 600\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "keras_fc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_fc_model.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "20"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keras_fc_model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'batch_normalization/gamma:0' shape=(100,) dtype=float32, numpy=\n array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       dtype=float32)>,\n <tf.Variable 'batch_normalization/beta:0' shape=(100,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)>,\n <tf.Variable 'batch_normalization/moving_mean:0' shape=(100,) dtype=float32, numpy=\n array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       dtype=float32)>,\n <tf.Variable 'batch_normalization/moving_variance:0' shape=(100,) dtype=float32, numpy=\n array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n       dtype=float32)>]"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch norm layer variables\n",
    "keras_fc_model.layers[2].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_fc_model.compile(optimizer =  tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                      loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "                      metrics = [tf.keras.metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USing tf.keras.metrics.AUC() give:\n",
    "\n",
    "# \"\"\"InvalidArgumentError:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] \n",
    "# [x (kerasmodel1/dense_3/BiasAdd:0) = ] [[0.7673769 2.2196033 1.78565049...]...] [y (metrics/auc/Cast_1/x:0) = ] [0]\n",
    "#\t [[{{node metrics/auc/assert_greater_equal/Assert/AssertGuard/else/_1/Assert}}]] [Op:__inference_distributed_function_103831]\"\"\"\n",
    "\n",
    "# To use this metric, the predictions have to be between 0 and 1, implies softmax needs to be used.\n",
    "                      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'kerasmodel1'"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_fc_model.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning rate\n",
    "    * Adam optimizer can do adaptive learning rate feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not setting EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_savedmodels = 'keras_savedmodels'\n",
    "\n",
    "if os.path.exists(keras_savedmodels):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(keras_savedmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_savemodel = tf.keras.callbacks.ModelCheckpoint(os.path.join(keras_savedmodels, 'model_{epoch}-{val_loss:.3f}.h5'), mode = 'min',\n",
    "                                                  monitor = 'val_loss',\n",
    "                                                 verbose = 1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_models_logs = 'keras_models_logs'\n",
    "\n",
    "if os.path.exists(keras_models_logs):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(keras_models_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_tboard = tf.keras.callbacks.TensorBoard(log_dir=keras_models_logs,histogram_freq=1, write_graph=True,\n",
    "                                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cblist = [cb_savemodel, cb_tboard]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs1=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train for 916 steps, validate for 84 steps\nEpoch 1/10\n910/916 [============================>.] - ETA: 0s - loss: 0.3041 - categorical_accuracy: 0.9111\nEpoch 00001: val_loss improved from inf to 0.21771, saving model to keras_savedmodels/model_1-0.218.h5\n916/916 [==============================] - 7s 8ms/step - loss: 0.3034 - categorical_accuracy: 0.9113 - val_loss: 0.2177 - val_categorical_accuracy: 0.9344\nEpoch 2/10\n910/916 [============================>.] - ETA: 0s - loss: 0.1610 - categorical_accuracy: 0.9516\nEpoch 00002: val_loss improved from 0.21771 to 0.16106, saving model to keras_savedmodels/model_2-0.161.h5\n916/916 [==============================] - 5s 5ms/step - loss: 0.1611 - categorical_accuracy: 0.9515 - val_loss: 0.1611 - val_categorical_accuracy: 0.9526\nEpoch 3/10\n907/916 [============================>.] - ETA: 0s - loss: 0.1176 - categorical_accuracy: 0.9635\nEpoch 00003: val_loss improved from 0.16106 to 0.15292, saving model to keras_savedmodels/model_3-0.153.h5\n916/916 [==============================] - 5s 5ms/step - loss: 0.1173 - categorical_accuracy: 0.9636 - val_loss: 0.1529 - val_categorical_accuracy: 0.9540\nEpoch 4/10\n912/916 [============================>.] - ETA: 0s - loss: 0.0904 - categorical_accuracy: 0.9716\nEpoch 00004: val_loss improved from 0.15292 to 0.13310, saving model to keras_savedmodels/model_4-0.133.h5\n916/916 [==============================] - 5s 5ms/step - loss: 0.0904 - categorical_accuracy: 0.9715 - val_loss: 0.1331 - val_categorical_accuracy: 0.9606\nEpoch 5/10\n914/916 [============================>.] - ETA: 0s - loss: 0.0745 - categorical_accuracy: 0.9764\nEpoch 00005: val_loss improved from 0.13310 to 0.13114, saving model to keras_savedmodels/model_5-0.131.h5\n916/916 [==============================] - 5s 5ms/step - loss: 0.0746 - categorical_accuracy: 0.9763 - val_loss: 0.1311 - val_categorical_accuracy: 0.9640\nEpoch 6/10\n912/916 [============================>.] - ETA: 0s - loss: 0.0608 - categorical_accuracy: 0.9806\nEpoch 00006: val_loss improved from 0.13114 to 0.12137, saving model to keras_savedmodels/model_6-0.121.h5\n916/916 [==============================] - 5s 5ms/step - loss: 0.0610 - categorical_accuracy: 0.9805 - val_loss: 0.1214 - val_categorical_accuracy: 0.9652\nEpoch 7/10\n914/916 [============================>.] - ETA: 0s - loss: 0.0546 - categorical_accuracy: 0.9821\nEpoch 00007: val_loss did not improve from 0.12137\n916/916 [==============================] - 5s 5ms/step - loss: 0.0545 - categorical_accuracy: 0.9822 - val_loss: 0.1346 - val_categorical_accuracy: 0.9658\nEpoch 8/10\n913/916 [============================>.] - ETA: 0s - loss: 0.0463 - categorical_accuracy: 0.9848\nEpoch 00008: val_loss did not improve from 0.12137\n916/916 [==============================] - 5s 5ms/step - loss: 0.0462 - categorical_accuracy: 0.9848 - val_loss: 0.1303 - val_categorical_accuracy: 0.9660\nEpoch 9/10\n912/916 [============================>.] - ETA: 0s - loss: 0.0410 - categorical_accuracy: 0.9863\nEpoch 00009: val_loss did not improve from 0.12137\n916/916 [==============================] - 5s 5ms/step - loss: 0.0411 - categorical_accuracy: 0.9862 - val_loss: 0.1264 - val_categorical_accuracy: 0.9670\nEpoch 10/10\n908/916 [============================>.] - ETA: 0s - loss: 0.0350 - categorical_accuracy: 0.9883\nEpoch 00010: val_loss improved from 0.12137 to 0.11878, saving model to keras_savedmodels/model_10-0.119.h5\n916/916 [==============================] - 5s 5ms/step - loss: 0.0349 - categorical_accuracy: 0.9884 - val_loss: 0.1188 - val_categorical_accuracy: 0.9702\n"
    }
   ],
   "source": [
    "keras_history1 = keras_fc_model.fit(train_dataset, epochs=epochs1, verbose = 1, callbacks=cblist,\n",
    "                                   validation_data=valid_dataset, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "167/167 [==============================] - 0s 3ms/step - loss: 0.1114 - categorical_accuracy: 0.9711\n"
    },
    {
     "data": {
      "text/plain": "[0.11143299131751797, 0.9711]"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_fc_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exploring Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f33f44998d0>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f340c27a8d0>,\n <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f340c228dd8>,\n <tensorflow.python.keras.layers.core.Activation at 0x7f340c228f60>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f340c1faeb8>,\n <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f340c1fe048>,\n <tensorflow.python.keras.layers.core.Activation at 0x7f340c1fe2b0>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f340c1a4d68>,\n <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7f340c1ac198>,\n <tensorflow.python.keras.layers.core.Activation at 0x7f340c1ac438>,\n <tensorflow.python.keras.layers.core.Dense at 0x7f340c1fab70>]"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_fc_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = keras_fc_model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'batch_normalization/gamma:0' shape=(100,) dtype=float32, numpy=\n array([1.2269483, 1.1349252, 1.3410976, 0.9179288, 1.2856462, 1.3862823,\n        1.3595926, 1.2745571, 1.5197902, 1.3166919, 1.4408834, 1.2638034,\n        1.2629784, 1.2740654, 1.415783 , 1.4326637, 1.4493729, 1.3970267,\n        1.3195894, 1.3734947, 1.3420596, 1.1721978, 1.2968619, 1.5249897,\n        1.0370381, 1.3912894, 1.5404277, 1.3771281, 1.4850557, 1.3972007,\n        1.4163164, 1.1762176, 1.1790816, 1.3884237, 1.3342997, 1.303082 ,\n        1.1480755, 1.2126306, 1.2919899, 1.1597459, 1.485534 , 1.2900969,\n        1.221908 , 1.2401632, 1.4433057, 1.4404896, 1.508135 , 1.3095189,\n        1.4880893, 1.3983922, 1.2017698, 1.2355157, 1.4851977, 1.5640281,\n        1.5173646, 1.3068354, 1.62453  , 1.5176066, 1.4924428, 1.2629938,\n        1.327193 , 1.4941337, 1.6335568, 1.4370104, 1.5315871, 1.3623064,\n        1.284233 , 1.1915743, 1.4893486, 1.3606577, 1.1597953, 1.3838465,\n        1.2619832, 1.2759168, 1.5620822, 1.2270515, 1.5284364, 1.2064832,\n        1.3470151, 1.4076749, 1.3609585, 1.3630152, 1.6558574, 1.6179266,\n        1.2785853, 1.1306003, 1.3955864, 1.4067265, 1.5867912, 1.4267366,\n        1.3758888, 1.4742678, 1.2202123, 1.0020919, 1.2884294, 1.423675 ,\n        1.3156143, 1.4312552, 1.415758 , 1.4510306], dtype=float32)>,\n <tf.Variable 'batch_normalization/beta:0' shape=(100,) dtype=float32, numpy=\n array([ 0.0725309 ,  0.13827226, -0.06939456,  0.12959993, -0.7255127 ,\n        -0.33006316, -0.07126302,  0.57206756, -0.02050218,  0.01971257,\n        -0.14860351,  0.14233395, -0.5124093 , -0.48969638, -0.34047705,\n        -0.1610549 ,  0.1095884 ,  0.22338244,  0.478024  ,  0.23863536,\n        -0.6544056 , -0.61530656,  0.43131748,  0.25865248, -0.05738853,\n        -0.19319026, -0.1278918 ,  0.20257951, -0.34966147, -0.22343421,\n        -0.28003773,  0.06845671, -0.18589702,  0.24006306, -0.13137347,\n        -0.18610711, -0.09652063,  0.07730827, -0.13674106, -0.36124447,\n        -0.2068936 ,  0.1938605 , -0.07766312,  0.31542403, -0.39941385,\n        -0.13848141, -0.24340703,  0.22813042,  0.3291286 , -0.40002584,\n        -0.39932287,  0.27706632,  0.41098887, -0.23980504, -0.19397306,\n         0.01697474,  0.17772289, -0.23378782, -0.08268411, -0.39916015,\n         0.5646259 ,  0.11323285, -0.29715964,  0.48129022, -0.3476707 ,\n        -0.19937208,  0.07189935, -0.1534567 , -0.239148  ,  0.3631805 ,\n         0.2086617 , -0.43803284,  0.36308923,  0.2902676 ,  0.5195069 ,\n        -0.12038191, -0.41274363, -0.03893393, -0.17440654,  0.27123487,\n        -0.19878754,  0.00880557, -0.51359844,  0.01657882, -0.18577358,\n         0.5174917 ,  0.00808138, -0.25319344, -0.13443038, -0.16466412,\n         0.4555682 ,  0.4716943 , -0.46857092, -0.19754733,  0.11025815,\n        -0.80802745,  0.20232354,  0.60316294,  0.07676253, -0.2766438 ],\n       dtype=float32)>,\n <tf.Variable 'batch_normalization/moving_mean:0' shape=(100,) dtype=float32, numpy=\n array([-9.47964936e-03, -1.13305561e-02, -2.65359320e-02,  1.08358869e-03,\n        -2.73278710e-02, -3.11168749e-03, -7.59504922e-03,  6.60309009e-03,\n         4.54964191e-02, -6.23745052e-03, -2.85433084e-02,  4.74635232e-03,\n        -1.88576318e-02,  2.24377774e-02,  5.44967549e-03, -4.31050472e-02,\n         1.90221705e-04, -6.47694916e-02,  2.13363282e-02, -2.00563092e-02,\n        -5.59500493e-02, -2.07057185e-02,  1.85878631e-02,  1.05127068e-02,\n        -2.63166633e-02,  1.57331135e-02,  1.02630835e-02,  4.82410332e-03,\n        -2.12563518e-02,  1.59315709e-02, -3.26706059e-02, -6.98142685e-05,\n        -7.33800139e-03, -1.47310020e-02,  1.48518737e-02, -5.48786521e-02,\n        -5.33562191e-02,  6.19289353e-02,  8.21460597e-03,  8.23432300e-03,\n        -1.64685398e-02, -4.61975485e-02,  4.18895744e-02, -3.18473727e-02,\n        -2.41696835e-03, -2.99536195e-02, -2.16178652e-02,  2.30958760e-02,\n        -2.75954381e-02, -1.89153329e-02,  2.34767981e-03,  7.07078679e-03,\n         4.26627770e-02, -5.05843237e-02,  7.47829769e-03,  4.66979817e-02,\n         6.60671107e-03,  7.11089559e-03,  6.16222806e-02,  2.83963196e-02,\n         2.39989739e-02, -2.10756809e-03, -4.02762555e-02, -1.02800550e-03,\n         1.32320039e-02,  5.29733375e-02,  1.30893067e-02, -1.21751130e-02,\n         2.57658795e-03, -5.31097241e-02,  1.52281215e-02,  2.39445362e-03,\n         1.92800257e-03, -7.33795464e-02,  2.03121584e-02, -9.88201052e-03,\n         4.01057228e-02, -2.86189560e-02,  4.13046107e-02,  4.34740968e-02,\n         1.47780683e-02, -2.54178140e-02, -1.52519392e-03,  4.88437191e-02,\n         9.54913255e-03,  4.79647778e-02, -5.81578352e-03, -4.73721744e-03,\n         1.91070717e-02, -2.10116152e-03, -1.37595655e-02,  8.68080929e-03,\n         3.35385129e-02,  1.28508126e-02,  3.66466679e-03, -8.42949469e-03,\n         3.74097340e-02,  3.71622294e-02, -3.23811844e-02, -2.26809038e-03],\n       dtype=float32)>,\n <tf.Variable 'batch_normalization/moving_variance:0' shape=(100,) dtype=float32, numpy=\n array([21.076986 , 16.843124 , 20.834011 , 24.038845 , 40.036083 ,\n        27.389042 , 21.442293 , 34.84173  , 34.220356 , 16.27224  ,\n        23.450117 , 17.219156 , 30.438965 , 20.586555 , 27.593739 ,\n        21.050295 , 23.088821 , 19.461712 , 39.986233 , 22.584919 ,\n        27.14464  , 24.02813  , 18.352482 , 23.075844 , 15.904074 ,\n        22.048882 , 26.024933 , 21.055819 , 22.949402 , 24.86849  ,\n        26.507677 , 20.462847 , 16.049488 , 17.37114  , 36.425774 ,\n        25.451834 , 25.587803 , 22.594397 , 16.445166 , 25.267582 ,\n        24.626537 , 23.133387 , 15.845271 , 15.977093 , 26.684368 ,\n        18.19778  , 24.9619   , 29.210503 , 34.12977  , 23.537685 ,\n        23.798798 , 21.184074 , 28.373373 , 26.478884 , 16.513794 ,\n        19.778822 , 27.966188 , 19.807663 , 23.658226 , 25.400915 ,\n        28.438196 , 24.261097 , 26.656666 , 18.69155  , 20.17362  ,\n        15.1383705, 14.348465 , 17.397808 , 25.866411 , 19.936626 ,\n        23.50689  , 16.371773 , 20.483273 , 21.956085 , 68.508286 ,\n        18.651371 , 31.916445 , 16.893143 , 25.34363  , 25.841879 ,\n        22.599669 , 24.660942 , 25.784834 , 30.056602 , 23.253328 ,\n        16.411661 , 23.85958  , 16.93443  , 24.809193 , 37.38166  ,\n        22.852282 , 68.457695 , 23.873999 , 15.34342  , 23.76838  ,\n        23.517864 , 28.502975 , 27.204023 , 24.015827 , 17.425133 ],\n       dtype=float32)>]"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Variable 'batch_normalization/gamma:0' shape=(100,) dtype=float32, numpy=\n array([1.2269483, 1.1349252, 1.3410976, 0.9179288, 1.2856462, 1.3862823,\n        1.3595926, 1.2745571, 1.5197902, 1.3166919, 1.4408834, 1.2638034,\n        1.2629784, 1.2740654, 1.415783 , 1.4326637, 1.4493729, 1.3970267,\n        1.3195894, 1.3734947, 1.3420596, 1.1721978, 1.2968619, 1.5249897,\n        1.0370381, 1.3912894, 1.5404277, 1.3771281, 1.4850557, 1.3972007,\n        1.4163164, 1.1762176, 1.1790816, 1.3884237, 1.3342997, 1.303082 ,\n        1.1480755, 1.2126306, 1.2919899, 1.1597459, 1.485534 , 1.2900969,\n        1.221908 , 1.2401632, 1.4433057, 1.4404896, 1.508135 , 1.3095189,\n        1.4880893, 1.3983922, 1.2017698, 1.2355157, 1.4851977, 1.5640281,\n        1.5173646, 1.3068354, 1.62453  , 1.5176066, 1.4924428, 1.2629938,\n        1.327193 , 1.4941337, 1.6335568, 1.4370104, 1.5315871, 1.3623064,\n        1.284233 , 1.1915743, 1.4893486, 1.3606577, 1.1597953, 1.3838465,\n        1.2619832, 1.2759168, 1.5620822, 1.2270515, 1.5284364, 1.2064832,\n        1.3470151, 1.4076749, 1.3609585, 1.3630152, 1.6558574, 1.6179266,\n        1.2785853, 1.1306003, 1.3955864, 1.4067265, 1.5867912, 1.4267366,\n        1.3758888, 1.4742678, 1.2202123, 1.0020919, 1.2884294, 1.423675 ,\n        1.3156143, 1.4312552, 1.415758 , 1.4510306], dtype=float32)>,\n <tf.Variable 'batch_normalization/beta:0' shape=(100,) dtype=float32, numpy=\n array([ 0.0725309 ,  0.13827226, -0.06939456,  0.12959993, -0.7255127 ,\n        -0.33006316, -0.07126302,  0.57206756, -0.02050218,  0.01971257,\n        -0.14860351,  0.14233395, -0.5124093 , -0.48969638, -0.34047705,\n        -0.1610549 ,  0.1095884 ,  0.22338244,  0.478024  ,  0.23863536,\n        -0.6544056 , -0.61530656,  0.43131748,  0.25865248, -0.05738853,\n        -0.19319026, -0.1278918 ,  0.20257951, -0.34966147, -0.22343421,\n        -0.28003773,  0.06845671, -0.18589702,  0.24006306, -0.13137347,\n        -0.18610711, -0.09652063,  0.07730827, -0.13674106, -0.36124447,\n        -0.2068936 ,  0.1938605 , -0.07766312,  0.31542403, -0.39941385,\n        -0.13848141, -0.24340703,  0.22813042,  0.3291286 , -0.40002584,\n        -0.39932287,  0.27706632,  0.41098887, -0.23980504, -0.19397306,\n         0.01697474,  0.17772289, -0.23378782, -0.08268411, -0.39916015,\n         0.5646259 ,  0.11323285, -0.29715964,  0.48129022, -0.3476707 ,\n        -0.19937208,  0.07189935, -0.1534567 , -0.239148  ,  0.3631805 ,\n         0.2086617 , -0.43803284,  0.36308923,  0.2902676 ,  0.5195069 ,\n        -0.12038191, -0.41274363, -0.03893393, -0.17440654,  0.27123487,\n        -0.19878754,  0.00880557, -0.51359844,  0.01657882, -0.18577358,\n         0.5174917 ,  0.00808138, -0.25319344, -0.13443038, -0.16466412,\n         0.4555682 ,  0.4716943 , -0.46857092, -0.19754733,  0.11025815,\n        -0.80802745,  0.20232354,  0.60316294,  0.07676253, -0.2766438 ],\n       dtype=float32)>]"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14265"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datascience)",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}